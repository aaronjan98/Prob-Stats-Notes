## Moment Generating Function

- **Recall**:
	- $X$ is a discrete random variable with range $D_{X}$ and PMF $D_{x}$
		- $\mathbb{E}(X) = \sum_{x \in D_{X}}x \mathbb{P}(x)$
	- $M_{x}(t) = \mathbb{E}(e^{tx}) \quad\mathrm{for}\quad \lvert t \rvert < \epsilon \text{ for some }\epsilon > 0$ is called the moment generating function of $x$.
		- $\mathbb{E}(\mu(x)) = \sum_{x \in D_{X}}\mu(x) \mathbb{P}(x)$

- **Rules**
	- $M_{x}'(t) = \frac{d}{dt} \sum e^{tx}\mathbb{P}(x)$
		- $= \sum \frac{d}{dt} \left( e^{tx}\mathbb{P}(x) \right)$
			- **N.B.**: can treat $\mathbb{P}(x)$ as a constant
		- $= \sum (e^{tx})' \mathbb{P}(x)$
	- recall:
		- $(e^t)' = e^t$
		- $(e^{2t})' = 2e^{2t}$
	- $M_{x}'(t) = \sum x\cdot e^{tx} \mathbb{P}(x)$
	- $M_{x}'(0) = \sum x e^{0 (x)} \mathbb{P}(x) = \sum x \mathbb{P}(x) = \mathbb{E}(x)$
	- $M_{x}'(0) = \mathbb{E}(x)$ which is the first non-central moment.
		- **N.B.**: diff. btwn central and non-central is ..
			- $M_{x}'(t) = \sum x (e^{tx})' \mathbb{P}(x)$
			- $M_{x}''(t)= \sum x^2 e^{tx} \mathbb{P}(x)$
			- $M_{x}''(0) = \sum x^2 \cdot e^0 \mathbb{P}(x)$
				- $= \sum x^2 \mathbb{P}(x) = \mathbb{E}(x^2)$ which is the 2nd non-central moment
		- **N.B.**: $\mathbb{E}(x^k)$ is the kth non-central moment
			- if you keep taking derivates
				- $M^k (t) = \sum x^k e^{tx} \mathbb{P}(x)$
				- $M^k (0) = \sum x^k e^0 \mathbb{P}(x) = \sum x^k \mathbb{P}(x)$
					- $= \mathbb{E}(X^k)$ which the kth non-central moment
		- we call mgf bc we use it to generate all the moments of rv x

- **Example**:
	- Suppose $x$ is a random variable with mgf:
		- $M_{x}(t) = \dfrac{1}{1-t}$
	- Find $\mathbb{E}(x); \mathbb{E}(x^2)$ and $\operatorname{Var}( x )$
		- **N.B.**: to find these, need to take derivative and plug in 0
	- $M_{x}'(t) = \left(  (1-t)^{-1} \right)'$
		- $= -1 (1-t)^{-2}(-1)$
		- $= \dfrac{1}{(1-t)^2}$
		- $M_{x}'(0) = \dfrac{1}{(1-0)^2} = 1 = \mathbb{E}(x)$
	- $M_{x}''(t) = \left( (1-t)^{-2} \right)'$
		- $= -2 (1-3)^{-3} ( -1)$
	- $M_{x}''(t) = \dfrac{2}{(1-t)^3}$
		- $M_{x}''(0) = \dfrac{2}{(1-0)^3} = 2 = \mathbb{E}(x^2)$
	- $\operatorname{Var}( x ) = \mathbb{E}(x^2) - (\mathbb{E}(x))^2$
		- $= 2 - (1)^2$
		- $= 2- 1$
	- $\operatorname{Var}( x ) = 1$
	- **N.B.**: will likely see on the exam

## Binomial Distribution

- **N.B.**: need to understand the Bernoulli experiment first
	- will use coin flip experiment but applies for any other Bernoulli experiment
	- by default, a heads is considered a success and tails failure

- **Bernoulli Experiment**
	- A Bernoulli experiment is a random experiment with only two possible outcomes denoted success or failure.

- **Example**
	- Flip a coin that has probability $P$ of landing on Head and $1-P$ of landing Tail.
	- Define the random variable $x$ as follows:
		- $x = \begin{cases} 1 & \text{if we get success} \\ 0 & \text{if we get failure} \end{cases}$
	- $i$.$e$.
		- $x = \begin{cases} 0 & P(x = 0) = P(x=\text{failure})=P(x=T)=1-P. \\ 1 & P(x=1)=P(x=\text{success})=P(x=H) = P \end{cases}$
	- What is the pmf of $x$. $$ \begin{array}{|c|c|} \hline x & 0 & 1 \\ \hline P(x) & 1-P & P \\ \hline \end{array} $$
		- **N.B.**: $0 \leq P \leq 1$
	- $\mathbb{E}(x) = 0 (1-P) + 1(P)$
		- $\mathbb{E}(x) = P$.
	- What is $\operatorname{Var}( x )$?
		- $\mathbb{E}(x^2) = 0^2 (1-P) + 1^2 (P)$
			- $= P$
		- $\operatorname{Var}( x ) = \mathbb{E}(x^2) - (\mathbb{E}(x))^2$
			- $= P - P^2$
			- $= P(1-P)$.

- compute mgf of the bernoulli
	- $M_{x}(t) = \mathbb{E}(e^{tx})$
		- $= e^{t (0)} (1-P) + e^{t (1)} P$
		- $= 1-P + Pe^t$

- **Recapitulate**
	- $x \leadsto \operatorname{Ber}(P)$
	1. pmf:
		$$ \begin{array}{|c|c|} \hline x & 0 & 1 \\ \hline P(x) & 1-P & P \\ \hline \end{array} $$
	2. $\mathbb{E}(x)=P; \quad \mathbb{E}(x^2) = P$
		- $\operatorname{Var}( x ) = P(1-P)$
	3. mgf of $x$ is $M_{x}(t) = 1-P + Pe^t$
	
- **N.B.**: binomial rv is a bernoulli experiment repeated $n$ times

- **Binomial Definition**
	- Suppose a Bernoulli experiment with probability of success $P$ has been repeated $n$ times
	- The random variable that counts the number of successes in the $n$ trials is known as Binomial random variable.

- **N.B.**: when solving a problem, need to ask qs if it's an experiment that repeats (counts the number of independent successes)
	- need to identify the Binomial Parameters
	1. Probability of success $P$
	2. number of trials
	3. independence among the trial

- **Example**: Flip a coin 10 times with probability success $P$. Let $x$ be the random variable that counts the number of success.
	- \# of trials = 10
	- $\mathbb{P}(\text{success}) = P$
	- **N.B.**: range of $x$: $0 \leq P \leq 1$
		- range of $x$ is $D_{x} = \{ 0, 1, 2,\dots 10 \}$
	- $\mathbb{P}(x=0) = (1-P)^{10}$
	- $\mathbb{P}(x=1) = \dbinom{10}{1} P^1 (1-P)^9$
		- **N.B.**: just one head in the remaining 9 tails
	- $\mathbb{P}(x = 2) = \dbinom{10}{2} P^2 (1-P)^8$
	- $\mathbb{P}(x = 3) = \dbinom{10}{3} P^3 (1-P)^7$

- **General Formula for Binomial Distribution**
	- In general: Suppose $x \leadsto$ Binomial $(P, n)$
		- $P$ stands for probability of success
		- $n$ number of trials
		- $x$ is the count of \# of successes in the $n$ trials
	1. Range of $x$: $X_{x} = \{ 0, 1, 2, \dots, n \}$
	2. The pmf of $x$ is $\mathbb{P}(x) = \dbinom{n}{x}P^x (1-P)^{n-x}; x=0, 1, 2, \dots , n$
		- **N.B.**: not convenient as a table here, but still a pmf
	3. $\mathbb{E}(x) = nP$
		- $\operatorname{Var}( x )= n P(1-P)$
	4. $M_{x}(t) = (1- p + p e^t)^n$
		- For bournellli it was: $M_{x}(t) = 1- p + p e^t$
	5. cdf of x
		- $F(x) = P(X \leq x)$
			- $= \sum_{i=0}^{\lfloor x \rfloor} \dbinom{n}{i} P^i (1-P)^{n-i}$
